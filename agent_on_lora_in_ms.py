# ======================== äº¤äº’å¼å…»ç”Ÿå»ºè®®åŠ©æ‰‹ ========================
import os
import numpy as np
import mindspore as ms
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import warnings
warnings.filterwarnings("ignore")

# ======================== é…ç½®é¡¹ï¼ˆéœ€ä¸è®­ç»ƒè„šæœ¬ä¿æŒä¸€è‡´ï¼‰ ========================
# 1. åŸºç¡€é…ç½®
LOCAL_MODEL_PATH = os.path.abspath("Qwen2.5-0.5B-Instruct")  # åŸºåº§æ¨¡å‹è·¯å¾„
FINETUNED_MODEL_PATH = os.path.abspath("./qwen0.5b_lora_finetune_mindspore271/health_advice_qwen0.5b_mindspore271_final")  # è®­ç»ƒå¥½çš„LoRAæ¨¡å‹è·¯å¾„
MAX_SEQ_LEN = 256  # ä¸è®­ç»ƒæ—¶ä¸€è‡´
MAX_NEW_TOKENS = 128  # ç”Ÿæˆå›å¤çš„æœ€å¤§é•¿åº¦
DEVICE = "cpu"

# 2. MindSporeç¯å¢ƒé…ç½®ï¼ˆä¿æŒä¸è®­ç»ƒä¸€è‡´ï¼‰
ms.set_context(mode=ms.PYNATIVE_MODE, device_target="CPU")
ms.set_seed(42)

# ======================== æ¨¡å‹åŠ è½½å·¥å…·ç±» ========================
class HealthAdviceAssistant:
    def __init__(self):
        # 1. åŠ è½½Tokenizer
        print("ğŸ”„ åŠ è½½Tokenizer...")
        self.tokenizer = AutoTokenizer.from_pretrained(
            FINETUNED_MODEL_PATH,
            trust_remote_code=True,
            padding_side="right",
            local_files_only=True,
            cache_dir=None
        )
        # å…œåº•è®¾ç½®ç‰¹æ®Štoken
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        self.tokenizer.add_special_tokens({
            "eos_token": "<|endoftext|>",
            "pad_token": "<|endoftext|>",
            "bos_token": "<|endoftext|>",
        })

        # 2. åŠ è½½åŸºåº§æ¨¡å‹ + LoRAæƒé‡
        print("ğŸ”„ åŠ è½½å¾®è°ƒåçš„LoRAæ¨¡å‹...")
        self.base_model = AutoModelForCausalLM.from_pretrained(
            LOCAL_MODEL_PATH,
            trust_remote_code=True,
            dtype=torch.float32,
            low_cpu_mem_usage=True,
            local_files_only=True,
            device_map=DEVICE
        )
        # åŠ è½½è®­ç»ƒå¥½çš„LoRAæƒé‡
        self.finetuned_model = PeftModel.from_pretrained(
            self.base_model,
            FINETUNED_MODEL_PATH,
            local_files_only=True
        )
        self.finetuned_model.eval()  # æ¨ç†æ¨¡å¼

        # 3. åˆ›å»ºMindSpore-PyTorchå…¼å®¹åŒ…è£…å™¨
        class InferenceWrapper:
            def __init__(self, pt_model, tokenizer):
                self.pt_model = pt_model
                self.tokenizer = tokenizer

            def set_train(self, mode):
                if mode:
                    self.pt_model.train()
                else:
                    self.pt_model.eval()

            def generate(self, input_ids, attention_mask=None, **kwargs):
                """é€‚é…MindSporeå¼ é‡ç±»å‹çš„ç”Ÿæˆæ¥å£"""
                # MindSpore Tensor -> numpy -> PyTorch Tensor
                input_ids_np = input_ids.asnumpy()
                attention_mask_np = attention_mask.asnumpy() if attention_mask is not None else None

                # è½¬æ¢ä¸ºPyTorch longç±»å‹
                pt_input_ids = torch.from_numpy(input_ids_np).long()
                pt_attention_mask = torch.from_numpy(attention_mask_np).long() if attention_mask_np is not None else None

                # ç”Ÿæˆå›å¤ï¼ˆç¦ç”¨æ¢¯åº¦ï¼‰
                with torch.no_grad():
                    outputs = self.pt_model.generate(
                        input_ids=pt_input_ids,
                        attention_mask=pt_attention_mask,** kwargs
                    )

                # PyTorch -> numpy -> MindSpore Tensor
                outputs_np = outputs.cpu().numpy().astype(np.int32)
                return ms.Tensor(outputs_np, ms.int32)

        self.model_wrapper = InferenceWrapper(self.finetuned_model, self.tokenizer)
        print("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼")

    def generate_advice(self, bad_habit: str) -> str:
        """
        ç”Ÿæˆå…»ç”Ÿå»ºè®®
        :param bad_habit: ç”¨æˆ·è¾“å…¥çš„ä¸è‰¯ç”Ÿæ´»ä¹ æƒ¯
        :return: æ ¼å¼åŒ–çš„å…»ç”Ÿå»ºè®®
        """
        # 1. æ„å»ºQwen2.5å¯¹è¯æ¨¡æ¿
        chat = [{"role": "user", "content": f"ç”¨æˆ·æœ‰ä¸è‰¯ç”Ÿæ´»ä¹ æƒ¯ï¼š{bad_habit}ï¼Œè¯·ç»™å‡ºå…»ç”Ÿå»ºè®®ã€‚"}]
        prompt = self.tokenizer.apply_chat_template(
            chat,
            tokenize=False,
            add_generation_prompt=True
        )

        # 2. ç¼–ç ä¸ºMindSporeå¼ é‡ï¼ˆé€‚é…è®­ç»ƒæ—¶çš„æ ¼å¼ï¼‰
        inputs = self.tokenizer(
            prompt,
            return_tensors="np",
            truncation=True,
            max_length=MAX_SEQ_LEN - MAX_NEW_TOKENS
        )
        input_ids_np = inputs["input_ids"].astype(np.int32)
        attention_mask_np = inputs["attention_mask"].astype(np.int32)
        input_ids = ms.Tensor(input_ids_np, ms.int32)
        attention_mask = ms.Tensor(attention_mask_np, ms.int32)

        # 3. ç”Ÿæˆå›å¤
        self.model_wrapper.set_train(False)
        outputs = self.model_wrapper.generate(
            input_ids=input_ids,
            attention_mask=attention_mask,
            max_new_tokens=MAX_NEW_TOKENS,
            temperature=0.7,        # æ§åˆ¶ç”Ÿæˆéšæœºæ€§
            top_p=0.9,              # æ ¸é‡‡æ ·
            repetition_penalty=1.15, # é¿å…é‡å¤
            do_sample=True,
            eos_token_id=self.tokenizer.eos_token_id,
            pad_token_id=self.tokenizer.pad_token_id,
            num_beams=1,
            no_repeat_ngram_size=3
        )

        # 4. è§£ç å¹¶æå–å›å¤
        response = self.tokenizer.decode(outputs.asnumpy().squeeze().tolist(), skip_special_tokens=True)
        advice = response.split("assistant\n")[-1].strip()

        # 5. å…œåº•å¤„ç†ï¼ˆä¿è¯å›å¤å®Œæ•´æ€§ï¼‰
        if len(advice) < 10 or not advice.endswith(("ï¼Œ", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼š")):
            advice += "\nğŸ’¡ è¡¥å……å»ºè®®ï¼šä¿æŒè§„å¾‹ä½œæ¯ã€å‡è¡¡é¥®é£Ÿï¼Œé€‚åº¦è¿åŠ¨ï¼Œå¢å¼ºèº«ä½“æŠµæŠ—åŠ›ã€‚"

        return advice

# ======================== äº¤äº’å¼å¯¹è¯ä¸»ç¨‹åº ========================
def main():
    # åˆå§‹åŒ–åŠ©æ‰‹
    try:
        assistant = HealthAdviceAssistant()
    except FileNotFoundError as e:
        print(f"âŒ æ¨¡å‹æ–‡ä»¶æœªæ‰¾åˆ°ï¼š{e}")
        print("è¯·ç¡®è®¤ï¼š")
        print(f"  1. åŸºåº§æ¨¡å‹è·¯å¾„ï¼š{LOCAL_MODEL_PATH} å­˜åœ¨")
        print(f"  2. å¾®è°ƒæ¨¡å‹è·¯å¾„ï¼š{FINETUNED_MODEL_PATH} å­˜åœ¨")
        return
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼š{e}")
        return

    # æ¬¢è¿è¯­
    print("\n" + "="*80)
    print("ğŸ¯ å…»ç”Ÿå»ºè®®æ™ºèƒ½åŠ©æ‰‹ï¼ˆåŸºäºQwen2.5-0.5B LoRAå¾®è°ƒï¼‰")
    print("="*80)
    print("ğŸ’¡ è¾“å…¥ä¸è‰¯ç”Ÿæ´»ä¹ æƒ¯ï¼Œæˆ‘ä¼šä¸ºæ‚¨æä¾›ä¸ªæ€§åŒ–å…»ç”Ÿå»ºè®®")
    print("ğŸ’¡ è¾“å…¥ 'é€€å‡º'/'quit'/'exit' å¯ç»“æŸå¯¹è¯")
    print("="*80 + "\n")

    # äº¤äº’å¼å¯¹è¯å¾ªç¯
    while True:
        # è·å–ç”¨æˆ·è¾“å…¥
        user_input = input("ğŸ§‘ æ‚¨çš„ä¸è‰¯ç”Ÿæ´»ä¹ æƒ¯ï¼š").strip()

        # é€€å‡ºæ¡ä»¶
        if user_input.lower() in ["é€€å‡º", "quit", "exit", "q"]:
            print("ğŸ‘‹ æ„Ÿè°¢ä½¿ç”¨ï¼Œç¥æ‚¨èº«ä½“å¥åº·ï¼")
            break

        # ç©ºè¾“å…¥å¤„ç†
        if not user_input:
            print("âš ï¸  è¯·è¾“å…¥æœ‰æ•ˆçš„ä¸è‰¯ç”Ÿæ´»ä¹ æƒ¯æè¿°ï¼ˆä¾‹å¦‚ï¼šæ¯å¤©ç†¬å¤œåˆ°å‡Œæ™¨1ç‚¹ï¼‰\n")
            continue

        # ç”Ÿæˆå¹¶å±•ç¤ºå»ºè®®
        try:
            print("ğŸ¤– å…»ç”Ÿå»ºè®®ï¼š", end="")
            advice = assistant.generate_advice(user_input)
            print(advice + "\n")
        except Exception as e:
            print(f"âŒ ç”Ÿæˆå»ºè®®å¤±è´¥ï¼š{e}\n")

if __name__ == "__main__":
    main()