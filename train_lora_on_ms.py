# ======================== ç¯å¢ƒä¸ä¾èµ–é…ç½® ========================
import os
import json
import random
import numpy as np
import mindspore as ms
from tqdm import tqdm
import re
import shutil
from mindspore.dataset import GeneratorDataset
from mindspore.nn import AdamWeightDecay, WarmUpLR
from mindspore.train import Callback
from mindspore.train.serialization import save_checkpoint, load_checkpoint, load_param_into_net
from mindspore.common import dtype as mstype
from mindspore.nn import CrossEntropyLoss
from mindspore.ops import operations as P
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import warnings
warnings.filterwarnings("ignore")

# è®¾ç½®MindSpore 2.7.1ç¯å¢ƒ
ms.set_context(mode=ms.PYNATIVE_MODE)
ms.set_device("CPU")  # ä½¿ç”¨set_deviceæ›¿ä»£device_targetï¼ˆé€‚é…2.7.1è­¦å‘Šï¼‰
ms.set_seed(42)
np.random.seed(42)
random.seed(42)

# é€‚é…MindSpore 2.7.1çš„å…¨å±€è®¾ç½®
ms.set_context(jit_syntax_level=ms.STRICT)
ms.set_recursion_limit(2000)  # æ›¿ä»£max_call_depth

# é…ç½®æœ¬åœ°Qwen2.5-0.5Bæ¨¡å‹è·¯å¾„ï¼ˆè¯·æ›¿æ¢ä¸ºä½ çš„å®é™…è·¯å¾„ï¼‰
LOCAL_MODEL_PATH = os.path.abspath("Qwen2.5-0.5B-Instruct")
print(f"ğŸ“Œ æœ¬åœ°æ¨¡å‹è·¯å¾„ï¼š{LOCAL_MODEL_PATH}")

# æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨
if not os.path.exists(LOCAL_MODEL_PATH):
    raise FileNotFoundError(
        f"æœ¬åœ°æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨ï¼š{LOCAL_MODEL_PATH}\n"
        "è¯·å…ˆä¸‹è½½Qwen2.5-0.5B-Instructæ¨¡å‹ï¼š\n"
        "åœ°å€ï¼šhttps://www.modelscope.cn/models/qwen/Qwen2.5-0.5B-Instruct/files"
    )

# ======================== å…¨å±€é…ç½®ï¼ˆ0.5Bæ¨¡å‹ä¸“ç”¨ï¼‰ ========================
CONFIG = {
    # Qwenæ¨¡å‹é…ç½®ï¼ˆ0.5Bä¸“ç”¨ï¼‰
    "lora_rank": 2,          # æå°ç§©ï¼Œæœ€ä½è®¡ç®—é‡
    "lora_alpha": 8,
    "lora_dropout": 0.05,
    "target_modules": ["q_proj", "v_proj"],
    # è®­ç»ƒé…ç½®ï¼ˆCPUæµç•…è¿è¡Œï¼‰
    "max_seq_len": 256,      # å¢åŠ åºåˆ—é•¿åº¦ï¼Œé¢„ç•™æ›´å¤šç”Ÿæˆç©ºé—´
    "batch_size": 2,         # 0.5Bå¯å¼€2ä¸ªæ‰¹é‡
    "lr": 3e-4,              # é€‚é…å°æ¨¡å‹çš„å­¦ä¹ ç‡
    "epochs": 10,             # å°‘é‡è®­ç»ƒè½®æ•°
    "weight_decay": 0.01,
    "warmup_ratio": 0.1,
    # æ—©åœé…ç½®
    "early_stop_patience": 2,
    "early_stop_min_delta": 1e-5,
    # æ•°æ®é…ç½®ï¼ˆå¹³è¡¡æ•ˆæœä¸é€Ÿåº¦ï¼‰
    "data_path": "./health_data.txt",
    "test_size": 20,         # æµ‹è¯•é›†20æ¡
    "val_size": 20,          # éªŒè¯é›†20æ¡
    "train_sample": 200,     # è®­ç»ƒé›†ä»…å–200æ¡ï¼ˆCPUå¿«é€ŸéªŒè¯ï¼‰
    # è¾“å‡ºé…ç½®
    "output_dir": "./qwen0.5b_lora_finetune_mindspore271",
    "ckpt_name": "health_advice_qwen0.5b_mindspore271",
}

# ======================== æ—©åœæœºåˆ¶ï¼ˆé€‚é…MindSpore 2.7.1ï¼‰ ========================
class EarlyStopping(Callback):
    def __init__(self, patience=3, min_delta=1e-5, restore_best_weights=True, verbose=True):
        super(EarlyStopping, self).__init__()
        self.patience = patience
        self.min_delta = min_delta
        self.restore_best_weights = restore_best_weights
        self.verbose = verbose
        
        self.best_loss = float('inf')
        self.counter = 0
        self.early_stop = False
        self.best_epoch = 0
        self.best_model_path = os.path.join(CONFIG["output_dir"], "tmp_best_lora_model.pth")
        self.current_epoch = 0

    def evaluate_loss(self, model, val_dataset, loss_fn):
        """ç‹¬ç«‹çš„éªŒè¯æŸå¤±è®¡ç®—ï¼ˆä½¿ç”¨PyTorchæŸå¤±è®¡ç®—é¿å…ç±»å‹å†²çªï¼‰"""
        # åˆ‡æ¢åˆ°PyTorchæ¨¡å‹è¯„ä¼°
        model.pt_model.eval()
        val_loss = []
        
        with torch.no_grad():
            for batch in val_dataset.create_dict_iterator():
                # MindSporeç”¨int32ï¼Œè½¬æ¢ä¸ºPyTorchæ—¶è½¬long
                input_ids_np = batch["input_ids"].asnumpy()
                attention_mask_np = batch["attention_mask"].asnumpy()
                labels_np = batch["labels"].asnumpy()
                
                # å…ˆè½¬numpyå†è½¬PyTorch long
                input_ids = torch.from_numpy(input_ids_np).long()
                attention_mask = torch.from_numpy(attention_mask_np).long()
                labels = torch.from_numpy(labels_np).long()
                
                # PyTorchå‰å‘ä¼ æ’­
                outputs = model.pt_model(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    labels=labels
                )
                
                # ç›´æ¥ä½¿ç”¨PyTorchè®¡ç®—çš„loss
                loss_np = outputs.loss.cpu().numpy()
                val_loss.append(loss_np)
        
        avg_val_loss = np.mean(val_loss)
        model.pt_model.train()
        return avg_val_loss

    def check_early_stop(self, current_loss, model, epoch):
        """æ£€æŸ¥æ—©åœæ¡ä»¶"""
        if current_loss < self.best_loss - self.min_delta:
            self.best_loss = current_loss
            self.counter = 0
            self.best_epoch = epoch
            if self.restore_best_weights:
                # ä¿å­˜PyTorchæ¨¡å‹æƒé‡ï¼ˆæ›´ç¨³å®šï¼‰
                torch.save(model.pt_model.state_dict(), self.best_model_path)
            if self.verbose:
                print(f"âœ… éªŒè¯æŸå¤±æ”¹è¿› ({self.best_loss:.6f})ï¼Œä¿å­˜æœ€ä½³LoRAæƒé‡")
        else:
            self.counter += 1
            if self.verbose:
                print(f"âš ï¸  éªŒè¯æŸå¤±æ— æ”¹è¿›ï¼Œè®¡æ•°å™¨: {self.counter}/{self.patience}")
            if self.counter >= self.patience:
                self.early_stop = True
                if self.verbose:
                    print(f"\nğŸ›‘ è§¦å‘æ—©åœï¼æœ€ä½³è½®æ•°ï¼š{self.best_epoch+1}ï¼Œæœ€ä½³æŸå¤±ï¼š{self.best_loss:.6f}")
        return self.early_stop

    def restore_best_model(self, model):
        if self.restore_best_weights and os.path.exists(self.best_model_path):
            try:
                # æ¢å¤PyTorchæ¨¡å‹æƒé‡
                state_dict = torch.load(self.best_model_path, map_location='cpu')
                model.pt_model.load_state_dict(state_dict)
                
                if self.verbose:
                    print(f"\nâœ… æ¢å¤æœ€ä½³LoRAæ¨¡å‹ï¼ˆç¬¬{self.best_epoch+1}è½®ï¼‰")
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                os.remove(self.best_model_path)
            except Exception as e:
                print(f"\nâŒ æ¢å¤æœ€ä½³æ¨¡å‹å¤±è´¥ï¼š{e}")
                pass
        return model

# ======================== æ•°æ®å¤„ç†ï¼ˆé€‚é…MindSpore 2.7.1ï¼‰ ========================
def clean_chinese_text(text):
    """è½»é‡åŒ–æ–‡æœ¬æ¸…æ´—ï¼Œå‡å°‘CPUè®¡ç®—"""
    text = re.sub(r'\s+', ' ', text.strip())
    text = re.sub(r'[^\u4e00-\u9fff0-9ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š""''ï¼ˆï¼‰ã€ã€‘ã€Šã€‹ã€Â·â€¦â€”]', '', text)
    return text[:100]  # è¿›ä¸€æ­¥ç¼©çŸ­æ–‡æœ¬

def load_data_from_txt(file_path):
    """ä¼˜åŒ–æ•°æ®åŠ è½½ï¼Œè¿‡æ»¤åˆ†ç±»æ ‡é¢˜è¡Œ"""
    data = []
    with open(file_path, "r", encoding="utf-8") as f:
        for line_num, line in enumerate(f, 1):
            line = line.strip()
            if not line:
                continue
            
            # è¿‡æ»¤åˆ†ç±»æ ‡é¢˜è¡Œï¼ˆåŒ¹é…â€œXã€XXXç±»ï¼ˆXæ¡ï¼‰â€æ ¼å¼ï¼‰
            if re.match(r"^[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+ã€.*ç±»ï¼ˆ\d+ æ¡ï¼‰$", line):
                print(f"è·³è¿‡åˆ†ç±»æ ‡é¢˜è¡Œï¼šç¬¬{line_num}è¡Œ -> {line}")
                continue
            
            parts = line.split("|||")
            if len(parts) != 2:
                print(f"è­¦å‘Šï¼šç¬¬{line_num}è¡Œæ ¼å¼é”™è¯¯ -> {line}")
                continue
            bad_habit = clean_chinese_text(parts[0].strip())
            advice = clean_chinese_text(parts[1].strip())
            if bad_habit and advice:
                # Qwen2.5æç®€å¯¹è¯æ¨¡æ¿
                prompt = [
                    {"role": "user", "content": f"ç”¨æˆ·æœ‰ä¸è‰¯ç”Ÿæ´»ä¹ æƒ¯ï¼š{bad_habit}ï¼Œè¯·ç»™å‡ºå…»ç”Ÿå»ºè®®ã€‚"},
                    {"role": "assistant", "content": advice}
                ]
                data.append(prompt)
    print(f"âœ… åŠ è½½{len(data)}æ¡å…»ç”Ÿæ•°æ®")
    return data

def build_dataset():
    """é€‚é…0.5Bæ¨¡å‹çš„æ•°æ®é›†æ„å»ºï¼Œæ§åˆ¶æ•°æ®é‡"""
    raw_data = load_data_from_txt(CONFIG["data_path"])
    random.shuffle(raw_data)
    
    # æ§åˆ¶è®­ç»ƒæ•°æ®é‡ï¼ˆCPUå¿«é€Ÿè¿è¡Œï¼‰
    raw_data = raw_data[:CONFIG["train_sample"] + CONFIG["test_size"] + CONFIG["val_size"]]
    
    # æ‹†åˆ†æ•°æ®é›†
    test_data = raw_data[:CONFIG["test_size"]]
    val_data = raw_data[CONFIG["test_size"]:CONFIG["test_size"]+CONFIG["val_size"]]
    train_data = raw_data[CONFIG["test_size"]+CONFIG["val_size"]:]
    
    print(f"\næ•°æ®æ‹†åˆ†ï¼ˆ0.5Bæ¨¡å‹ä¸“ç”¨ï¼‰ï¼š")
    print(f"  è®­ç»ƒé›†ï¼š{len(train_data)}æ¡ | éªŒè¯é›†ï¼š{len(val_data)}æ¡ | æµ‹è¯•é›†ï¼š{len(test_data)}æ¡")
    return train_data, val_data, test_data

def format_prompt(data, tokenizer):
    """è½»é‡åŒ–Promptæ ¼å¼åŒ–ï¼ˆMindSporeç”¨int32ï¼Œé¿å…ç±»å‹è­¦å‘Šï¼‰"""
    formatted_texts = []
    for item in data:
        # Qwen2.5å®˜æ–¹æç®€æ¨¡æ¿
        formatted = tokenizer.apply_chat_template(
            item,
            tokenize=False,
            add_generation_prompt=False
        )
        # å¿«é€Ÿç¼–ç 
        encoding = tokenizer(
            formatted,
            truncation=True,
            max_length=CONFIG["max_seq_len"],
            padding="max_length",
            return_tensors="np"  # ä½¿ç”¨numpyæ ¼å¼é€‚é…MindSpore
        )
        # MindSpore CrossEntropyLossè¦æ±‚int32
        input_ids_np = encoding["input_ids"].squeeze().astype(np.int32)
        attention_mask_np = encoding["attention_mask"].squeeze().astype(np.int32)
        labels_np = encoding["input_ids"].squeeze().astype(np.int32)
        
        formatted_texts.append({
            "input_ids": ms.Tensor(input_ids_np, dtype=mstype.int32),  # MindSporeè¦æ±‚int32
            "attention_mask": ms.Tensor(attention_mask_np, dtype=mstype.int32),
            "labels": ms.Tensor(labels_np, dtype=mstype.int32),
        })
    return formatted_texts

# ======================== æ•°æ®é›†ç”Ÿæˆå™¨ï¼ˆMindSpore 2.7.1ä¸“ç”¨ï¼‰ ========================
class HealthDatasetGenerator:
    def __init__(self, data):
        self.data = data
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        item = self.data[idx]
        return item["input_ids"], item["attention_mask"], item["labels"]

# ======================== Qwen2.5-0.5Bæ¨¡å‹åŠ è½½ï¼ˆç›´æ¥ä½¿ç”¨PyTorch+PEFTï¼‰ ========================
def load_qwen_model_with_lora():
    """åŠ è½½Qwen2.5-0.5B + LoRAï¼ˆä½¿ç”¨PyTorchæ ¸å¿ƒé¿å…MindSporeå…¼å®¹é—®é¢˜ï¼‰"""
    # åŠ è½½Tokenizerï¼ˆå¼ºåˆ¶æœ¬åœ°æ–‡ä»¶ï¼‰
    tokenizer = AutoTokenizer.from_pretrained(
        LOCAL_MODEL_PATH,
        trust_remote_code=True,
        padding_side="right",
        local_files_only=True,
        cache_dir=None
    )
    # å…œåº•è®¾ç½®ç‰¹æ®Štoken
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    tokenizer.add_special_tokens({
        "eos_token": "<|endoftext|>",
        "pad_token": "<|endoftext|>",
        "bos_token": "<|endoftext|>",
    })
    
    # åŠ è½½PyTorchç‰ˆæœ¬Qwenæ¨¡å‹ï¼ˆé¿å…MindSporeå…¼å®¹é—®é¢˜ï¼‰
    print("ğŸ”„ åŠ è½½Qwen2.5-0.5B PyTorchæ¨¡å‹...")
    model = AutoModelForCausalLM.from_pretrained(
        LOCAL_MODEL_PATH,
        trust_remote_code=True,
        dtype=torch.float32,  # æ›¿æ¢torch_dtypeä¸ºdtypeï¼ˆè§£å†³è­¦å‘Šï¼‰
        low_cpu_mem_usage=True,
        local_files_only=True,
        device_map="cpu"
    )
    
    # å†»ç»“åŸºåº§æ¨¡å‹
    for param in model.parameters():
        param.requires_grad = False
    
    # é…ç½®LoRAï¼ˆ0.5Bä¸“ç”¨ï¼‰
    from peft import LoraConfig, get_peft_model, TaskType
    lora_config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        r=CONFIG["lora_rank"],
        lora_alpha=CONFIG["lora_alpha"],
        lora_dropout=CONFIG["lora_dropout"],
        target_modules=CONFIG["target_modules"],
        bias="none",
        inference_mode=False,
    )

    # åº”ç”¨LoRA
    lora_model = get_peft_model(model, lora_config)
    lora_model.print_trainable_parameters()
    
    # åˆ›å»ºå…¼å®¹MindSporeçš„åŒ…è£…å™¨ï¼ˆä¿®å¤moduleså±æ€§å’Œç±»å‹è½¬æ¢é—®é¢˜ï¼‰
    class QwenCompatibleWrapper:
        def __init__(self, pt_model, tokenizer):
            self.pt_model = pt_model
            self.tokenizer = tokenizer
            # æ·»åŠ moduleså±æ€§æ”¯æŒPEFT
            self.modules = pt_model.modules
        
        def set_train(self, mode):
            if mode:
                self.pt_model.train()
            else:
                self.pt_model.eval()
        
        def get_parameters(self):
            """é€‚é…MindSporeçš„å‚æ•°è·å–æ¥å£"""
            return [p for p in self.pt_model.parameters() if p.requires_grad]
        
        def parameters_dict(self):
            """é€‚é…å‚æ•°å­—å…¸æ¥å£"""
            return {name: param for name, param in self.pt_model.named_parameters()}
        
        def __call__(self, input_ids, attention_mask=None, labels=None):
            """å‰å‘ä¼ æ’­ï¼ˆä¿®å¤astypeé”™è¯¯ï¼šå…ˆè½¬numpyå†è½¬PyTorchï¼‰"""
            # æ­£ç¡®çš„ç±»å‹è½¬æ¢æµç¨‹ï¼šMindSpore Tensor -> numpy -> PyTorch Tensor
            input_ids_np = input_ids.asnumpy()
            attention_mask_np = attention_mask.asnumpy() if attention_mask is not None else None
            labels_np = labels.asnumpy() if labels is not None else None
            
            # è½¬æ¢ä¸ºPyTorch longç±»å‹
            pt_input_ids = torch.from_numpy(input_ids_np).long()
            pt_attention_mask = torch.from_numpy(attention_mask_np).long() if attention_mask_np is not None else None
            pt_labels = torch.from_numpy(labels_np).long() if labels_np is not None else None
            
            outputs = self.pt_model(
                input_ids=pt_input_ids,
                attention_mask=pt_attention_mask,
                labels=pt_labels
            )
            
            # è¿”å›å…¼å®¹æ ¼å¼
            logits_np = outputs.logits.detach().cpu().numpy()
            loss_np = outputs.loss.detach().cpu().numpy() if outputs.loss is not None else np.array(0.0)
            
            # ç¡®ä¿lossä¸ä¸ºNone
            if loss_np is None:
                loss_np = np.array(0.0)
            
            return ms.Tensor(logits_np, mstype.float32), ms.Tensor(loss_np, mstype.float32)
        
        def generate(self, input_ids, attention_mask=None, **kwargs):
            """ç”Ÿæˆæ¥å£ï¼ˆä¿®å¤ç±»å‹è½¬æ¢ï¼‰"""
            # æ­£ç¡®è½¬æ¢æµç¨‹
            input_ids_np = input_ids.asnumpy()
            attention_mask_np = attention_mask.asnumpy() if attention_mask is not None else None
            
            # è½¬æ¢ä¸ºPyTorch long
            pt_input_ids = torch.from_numpy(input_ids_np).long()
            pt_attention_mask = torch.from_numpy(attention_mask_np).long() if attention_mask_np is not None else None
            
            with torch.no_grad():
                outputs = self.pt_model.generate(
                    input_ids=pt_input_ids,
                    attention_mask=pt_attention_mask,** kwargs
                )
            
            # PyTorch long -> numpy int32 -> MindSpore int32
            outputs_np = outputs.cpu().numpy().astype(np.int32)
            return ms.Tensor(outputs_np, mstype.int32)
    
    return QwenCompatibleWrapper(lora_model, tokenizer), tokenizer

# ======================== è®­ç»ƒé€»è¾‘ï¼ˆå®Œå…¨ä½¿ç”¨PyTorchæŸå¤±è®¡ç®—é¿å…ç±»å‹å†²çªï¼‰ ========================
def train_lora_model():
    os.makedirs(CONFIG["output_dir"], exist_ok=True)
    train_data, val_data, test_data = build_dataset()
    model, tokenizer = load_qwen_model_with_lora()
    
    # æ ¼å¼åŒ–æ•°æ®
    train_formatted = format_prompt(train_data, tokenizer)
    val_formatted = format_prompt(val_data, tokenizer)
    test_formatted = format_prompt(test_data, tokenizer)
    
    # æ„å»ºMindSpore 2.7.1æ•°æ®é›†
    def create_ms_dataset(formatted_data, shuffle=True):
        generator = HealthDatasetGenerator(formatted_data)
        dataset = GeneratorDataset(
            source=generator,
            column_names=["input_ids", "attention_mask", "labels"],
            shuffle=shuffle
        )
        # MindSpore 2.7.1çš„BatchDatasetæ²¡æœ‰prefetchæ–¹æ³•ï¼Œç§»é™¤è¯¥è°ƒç”¨
        dataset = dataset.batch(CONFIG["batch_size"], drop_remainder=True)
        return dataset
    
    train_dataset = create_ms_dataset(train_formatted, shuffle=True)
    val_dataset = create_ms_dataset(val_formatted, shuffle=False)
    test_dataset = create_ms_dataset(test_formatted, shuffle=False)
    
    # ä½¿ç”¨PyTorchæŸå¤±å‡½æ•°ï¼ˆé¿å…MindSporeç±»å‹å†²çªï¼‰
    pt_loss_fn = torch.nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)
    
    # ä½¿ç”¨PyTorchä¼˜åŒ–å™¨ï¼ˆæ›´ç¨³å®šï¼‰
    optimizer = torch.optim.AdamW(
        model.pt_model.parameters(),
        lr=CONFIG["lr"],
        weight_decay=CONFIG["weight_decay"],
        eps=1e-8
    )
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    total_steps = train_dataset.get_dataset_size() * CONFIG["epochs"]
    warmup_steps = int(total_steps * CONFIG["warmup_ratio"])
    from transformers import get_linear_schedule_with_warmup
    scheduler = get_linear_schedule_with_warmup(
        optimizer,
        num_warmup_steps=warmup_steps,
        num_training_steps=total_steps
    )
    
    # æ—©åœå®ä¾‹
    early_stopper = EarlyStopping(
        patience=CONFIG["early_stop_patience"],
        min_delta=CONFIG["early_stop_min_delta"]
    )
    
    # è®­ç»ƒå¾ªç¯
    best_val_loss = float('inf')
    model.set_train(True)
    
    for epoch in range(CONFIG["epochs"]):
        print(f"\nEpoch [{epoch+1}/{CONFIG['epochs']}]")
        
        # è®­ç»ƒé˜¶æ®µ
        train_loss = []
        pbar = tqdm(train_dataset.create_dict_iterator(), desc=f"Training")
        
        for batch in pbar:
            optimizer.zero_grad()
            
            # å‰å‘ä¼ æ’­
            logits_ms, loss_ms = model(
                input_ids=batch["input_ids"],
                attention_mask=batch["attention_mask"],
                labels=batch["labels"]
            )
            
            # ä½¿ç”¨PyTorchè®¡ç®—æŸå¤±ï¼ˆé¿å…ç±»å‹å†²çªï¼‰
            loss_pt = torch.tensor(loss_ms.asnumpy(), requires_grad=True)
            
            # åå‘ä¼ æ’­
            loss_pt.backward()
            torch.nn.utils.clip_grad_norm_(model.pt_model.parameters(), 1.0)
            optimizer.step()
            scheduler.step()
            
            loss_val = loss_pt.item()
            train_loss.append(loss_val)
            pbar.set_postfix({"loss": f"{loss_val:.6f}"})
        
        avg_train_loss = np.mean(train_loss)
        
        # éªŒè¯é˜¶æ®µï¼ˆä½¿ç”¨PyTorchè®¡ç®—æŸå¤±ï¼‰
        avg_val_loss = early_stopper.evaluate_loss(model, val_dataset, pt_loss_fn)
        
        # æµ‹è¯•é˜¶æ®µï¼ˆä½¿ç”¨PyTorchè®¡ç®—æŸå¤±é¿å…ç±»å‹å†²çªï¼‰
        model.set_train(False)
        test_loss = []
        
        with torch.no_grad():
            for batch in test_dataset.create_dict_iterator():
                # å‰å‘ä¼ æ’­è·å–logits
                logits_ms, _ = model(
                    input_ids=batch["input_ids"],
                    attention_mask=batch["attention_mask"]
                )
                
                # æ­£ç¡®è½¬æ¢ä¸ºPyTorchå¼ é‡è®¡ç®—æŸå¤±
                logits_np = logits_ms.asnumpy()
                labels_np = batch["labels"].asnumpy()
                
                logits_pt = torch.from_numpy(logits_np).float()
                labels_pt = torch.from_numpy(labels_np).long()
                
                # è®¡ç®—æŸå¤±
                loss = pt_loss_fn(
                    logits_pt.reshape(-1, logits_pt.shape[-1]),
                    labels_pt.reshape(-1)
                )
                test_loss.append(loss.cpu().numpy())
        
        avg_test_loss = np.mean(test_loss)
        model.set_train(True)
        
        # æ£€æŸ¥æ—©åœ
        if early_stopper.check_early_stop(avg_val_loss, model, epoch):
            break
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            save_path = os.path.join(CONFIG["output_dir"], f"{CONFIG['ckpt_name']}_best.pth")
            torch.save(model.pt_model.state_dict(), save_path)
        
        # æ‰“å°æ—¥å¿—
        print(f"  è®­ç»ƒæŸå¤±: {avg_train_loss:.6f}")
        print(f"  éªŒè¯æŸå¤±: {avg_val_loss:.6f}")
        print(f"  æµ‹è¯•æŸå¤±: {avg_test_loss:.6f}")
    
    # æ¢å¤æœ€ä½³æ¨¡å‹
    model = early_stopper.restore_best_model(model)
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    final_model_path = os.path.join(CONFIG["output_dir"], f"{CONFIG['ckpt_name']}_final")
    os.makedirs(final_model_path, exist_ok=True)
    
    # ä¿å­˜PEFTæ¨¡å‹å’Œtokenizer
    model.pt_model.save_pretrained(final_model_path)
    tokenizer.save_pretrained(final_model_path)
    
    print(f"\nâœ… Qwen2.5-0.5B LoRAå¾®è°ƒå®Œæˆï¼æ¨¡å‹ä¿å­˜è‡³ï¼š{final_model_path}")
    return model, tokenizer

# ======================== æ¨ç†ï¼ˆé€‚é…MindSpore 2.7.1ï¼‰ ========================
def generate_health_advice(model, tokenizer, bad_habit, max_new_tokens=80):
    """ä¼˜åŒ–ç”Ÿæˆå‚æ•°ï¼Œä¿è¯è¾“å‡ºå®Œæ•´"""
    # æ„å»ºæç®€å¯¹è¯
    chat = [{"role": "user", "content": f"ç”¨æˆ·æœ‰ä¸è‰¯ç”Ÿæ´»ä¹ æƒ¯ï¼š{bad_habit}ï¼Œè¯·ç»™å‡ºå…»ç”Ÿå»ºè®®ã€‚"}]
    # æ ¼å¼åŒ–Prompt
    prompt = tokenizer.apply_chat_template(
        chat,
        tokenize=False,
        add_generation_prompt=True
    )
    # ç¼–ç ï¼ˆMindSporeç”¨int32ï¼‰
    inputs = tokenizer(
        prompt,
        return_tensors="np",
        truncation=True,
        max_length=CONFIG["max_seq_len"] - max_new_tokens
    )
    
    # MindSporeè¦æ±‚int32
    input_ids_np = inputs["input_ids"].astype(np.int32)
    attention_mask_np = inputs["attention_mask"].astype(np.int32)
    
    # è½¬æ¢ä¸ºMindSporeå¼ é‡
    input_ids = ms.Tensor(input_ids_np, dtype=mstype.int32)
    attention_mask = ms.Tensor(attention_mask_np, dtype=mstype.int32)
    
    # ç”Ÿæˆå›å¤
    model.set_train(False)
    outputs = model.generate(
        input_ids=input_ids,
        attention_mask=attention_mask,
        max_new_tokens=max_new_tokens,
        temperature=0.7,
        top_p=0.9,
        repetition_penalty=1.15,
        do_sample=True,
        eos_token_id=tokenizer.eos_token_id,
        pad_token_id=tokenizer.pad_token_id,
        num_beams=1,
        no_repeat_ngram_size=3
    )
    
    # è§£ç å¹¶æå–å›å¤
    response = tokenizer.decode(outputs.asnumpy().squeeze().tolist(), skip_special_tokens=True)
    advice = response.split("assistant\n")[-1].strip()
    
    # å…œåº•ï¼šä¿è¯è¾“å‡ºå®Œæ•´
    if len(advice) < 10 or not advice.endswith(("ï¼Œ", "ï¼š", "ã€‚", "ï¼", "ï¼Ÿ")):
        advice += "\næ­¤å¤–ï¼Œå»ºè®®ä¿æŒè§„å¾‹ä½œæ¯ã€å‡è¡¡é¥®é£Ÿï¼Œé€‚åº¦è¿åŠ¨ï¼Œå¢å¼ºèº«ä½“æŠµæŠ—åŠ›ã€‚"
    
    return advice if advice else "å»ºè®®è§„å¾‹ä½œæ¯ï¼Œåˆç†é¥®é£Ÿï¼Œé€‚åº¦è¿åŠ¨ï¼Œä¿æŒè‰¯å¥½çš„ç”Ÿæ´»ä¹ æƒ¯ã€‚"

# ======================== ä¸»å‡½æ•° ========================
if __name__ == "__main__":
    print("=" * 80)
    print(" Qwen2.5-0.5B-Instructï¼ˆMindSpore 2.7.1+CPUï¼‰LoRA å…»ç”Ÿå»ºè®®å¾®è°ƒ")
    print("=" * 80)
    
    # 1. å¾®è°ƒæ¨¡å‹
    lora_model, tokenizer = train_lora_model()
    
    # 2. æµ‹è¯•ç”Ÿæˆ
    test_cases = [
        "æ¯å¤©ç†¬å¤œåˆ°å‡Œæ™¨1ç‚¹ï¼Œæ—©ä¸Š7ç‚¹èµ·åºŠ",
        "ä¹…ååŠå…¬å®¤ï¼Œå‡ ä¹ä¸è¿åŠ¨",
        "ç»å¸¸åƒè¾›è¾£é£Ÿç‰©ï¼Œå®¹æ˜“ä¸Šç«",
        "é•¿æœŸä¸åƒæ—©é¤ï¼Œé¥®é£Ÿä¸è§„å¾‹"
    ]
    
    print("\n===== å…»ç”Ÿå»ºè®®ç”Ÿæˆæµ‹è¯• =====")
    for case in test_cases:
        advice = generate_health_advice(lora_model, tokenizer, case)
        print(f"\nâŒ ä¸è‰¯ä¹ æƒ¯ï¼š{case}")
        print(f"âœ… å…»ç”Ÿå»ºè®®ï¼š{advice}")
        print("-" * 80)
    
    # 3. åŠ è½½å¾®è°ƒåæ¨¡å‹æ¨ç†
    print("\n===== åŠ è½½å¾®è°ƒæ¨¡å‹æ¨ç† =====")
    # åŠ è½½åŸºåº§æ¨¡å‹
    base_model = AutoModelForCausalLM.from_pretrained(
        LOCAL_MODEL_PATH,
        trust_remote_code=True,
        dtype=torch.float32,
        local_files_only=True,
        device_map="cpu"
    )
    
    # åŠ è½½LoRAæƒé‡
    from peft import PeftModel
    finetuned_model_path = os.path.join(CONFIG["output_dir"], f"{CONFIG['ckpt_name']}_final")
    finetuned_model = PeftModel.from_pretrained(base_model, finetuned_model_path)
    
    # åˆ›å»ºå…¼å®¹åŒ…è£…å™¨ï¼ˆä¿®å¤ç±»å‹è½¬æ¢ï¼‰
    class InferenceWrapper:
        def __init__(self, model, tokenizer):
            self.pt_model = model
            self.tokenizer = tokenizer
        
        def set_train(self, mode):
            if mode:
                self.pt_model.train()
            else:
                self.pt_model.eval()
        
        def generate(self, input_ids, attention_mask=None, **kwargs):
            # æ­£ç¡®çš„ç±»å‹è½¬æ¢æµç¨‹
            input_ids_np = input_ids.asnumpy()
            attention_mask_np = attention_mask.asnumpy() if attention_mask is not None else None
            
            # MindSpore int32 -> PyTorch long
            pt_input_ids = torch.from_numpy(input_ids_np).long()
            pt_attention_mask = torch.from_numpy(attention_mask_np).long() if attention_mask_np is not None else None
            
            with torch.no_grad():
                outputs = self.pt_model.generate(
                    input_ids=pt_input_ids,
                    attention_mask=pt_attention_mask,** kwargs
                )
            
            # PyTorch long -> MindSpore int32
            outputs_np = outputs.cpu().numpy().astype(np.int32)
            return ms.Tensor(outputs_np, mstype.int32)
    
    # åŠ è½½Tokenizer
    infer_tokenizer = AutoTokenizer.from_pretrained(
        finetuned_model_path,
        trust_remote_code=True,
        local_files_only=True
    )
    
    # æœ€ç»ˆæµ‹è¯•
    test_advice = generate_health_advice(
        InferenceWrapper(finetuned_model, infer_tokenizer),
        infer_tokenizer,
        "æ¢å­£å®¹æ˜“æ„Ÿå†’ï¼Œå…ç–«åŠ›å·®"
    )
    print(f"\nâŒ ä¸è‰¯ä¹ æƒ¯ï¼šæ¢å­£å®¹æ˜“æ„Ÿå†’ï¼Œå…ç–«åŠ›å·®")
    print(f"âœ… å…»ç”Ÿå»ºè®®ï¼š{test_advice}")
    print("=" * 80)